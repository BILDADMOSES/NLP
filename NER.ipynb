{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style = \"text-align: center\" >NAMED ENTITY RECOGNITION</p>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **entity** refers to a specific, named object, concept, or item in a text, such as a person's name, a location, a date, a product, an organization, or any other distinct and recognizable element.\n",
    "\n",
    "\n",
    "## Using NLTK for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('Nairobi', 'NNP'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('like', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('ride', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Metro', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('visit', 'VB'),\n",
       " ('Juja', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('some', 'DT'),\n",
       " ('restraurants', 'NNS'),\n",
       " ('around', 'IN'),\n",
       " ('Kenyatta', 'NNP'),\n",
       " ('University', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "sentence = '''In Nairobi, I like to ride the Metro to visit Juja and some restraurants around Kenyatta University.'''\n",
    "\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent) #Will add tags for proper nouns, pronouns, adjectives,verbs that nltk uses based on English grammar\n",
    "tagged_sent[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the below abbreviations stand for the parts of speech:\n",
    "\n",
    "'like' (VBP) - VBP stands for Verb, Present Tense. In this context, \"like\" is a verb in the present tense.\n",
    "\n",
    "'to' (TO) - TO is a preposition or infinitive marker. It's often used before verbs to indicate an action.\n",
    "\n",
    "'ride' (VB) - VB stands for Verb Base Form. \"Ride\" is the base form of the verb.\n",
    "\n",
    "'the' (DT) - DT is a determiner. \"The\" is a definite article used before nouns.\n",
    "\n",
    "'Metro' (NNP) - NNP stands for Proper Noun, Singular. \"Metro\" is a proper noun, typically referring to a specific entity or place.\n",
    "\n",
    "\n",
    "We then pass the tagged sentence in to the named entity chunk function(<span style = \"color:blue\">ne_chunk </span>) which returns the sentence as a tree.\n",
    "\n",
    "### Named Entity Chunk Function\n",
    "\n",
    "The Named Entity Chunk function is part of natural language processing and is used to identify and extract named entities from text. \n",
    "\n",
    "\n",
    "Named entities are specific words or phrases that refer to particular types of entities, such as names of people, organizations, locations, dates, monetary values, and more. \n",
    "\n",
    "\n",
    "The Named Entity Chunk function groups these entities together in a way that makes it easier to analyze and understand the content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  (GPE Nairobi/NNP)\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  ride/VB\n",
      "  the/DT\n",
      "  (ORGANIZATION Metro/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (PERSON Juja/NNP)\n",
      "  and/CC\n",
      "  some/DT\n",
      "  restraurants/NNS\n",
      "  around/IN\n",
      "  (PERSON Kenyatta/NNP University/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "\n",
    "We use Bag-of-words to find topics in a text. You first create tokens using tokenization and then count all tokens you have. The theory is that the more frequent a word or token is, the more important it might be in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'to': 2,\n",
       "         'In': 1,\n",
       "         'Nairobi': 1,\n",
       "         ',': 1,\n",
       "         'I': 1,\n",
       "         'like': 1,\n",
       "         'ride': 1,\n",
       "         'the': 1,\n",
       "         'Metro': 1,\n",
       "         'visit': 1,\n",
       "         'Juja': 1,\n",
       "         'and': 1,\n",
       "         'some': 1,\n",
       "         'restraurants': 1,\n",
       "         'around': 1,\n",
       "         'Kenyatta': 1,\n",
       "         'University': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Frequency = Counter(word_tokenize(sentence))\n",
    "Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 2), ('In', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(Frequency).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual NER with polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polyglot\n",
      "  Downloading polyglot-16.7.4.tar.gz (126 kB)\n",
      "     -------------------------------------- 126.3/126.3 kB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: polyglot\n",
      "  Building wheel for polyglot (setup.py): started\n",
      "  Building wheel for polyglot (setup.py): finished with status 'done'\n",
      "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52604 sha256=7347b221f6020afd6dccf5f67e655bdf56f2f1d21194559757ee0e0b6b52a853\n",
      "  Stored in directory: c:\\users\\bildad otieno\\appdata\\local\\pip\\cache\\wheels\\77\\4a\\9d\\5141018da475375d91dc1af07520b1f2b077579f2f55353afb\n",
      "Successfully built polyglot\n",
      "Installing collected packages: polyglot\n",
      "Successfully installed polyglot-16.7.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - polyglot\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/conda-forge/win-64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://repo.anaconda.com/pkgs/main/win-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/win-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install polyglot\n",
    "#from polyglot.text import Text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
